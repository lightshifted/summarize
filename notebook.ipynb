{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f82bbd11",
   "metadata": {},
   "source": [
    "# GPT2 for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a15c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from minigpt.utils import set_seed\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc980156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bpe is short for Byte Pair Encoder. It translates arbitrary utf-8 strings into\n",
    "sequences of integers, where each integer represents small chunks of commonly\n",
    "occuring characters. This implementation is based on openai's gpt2 encoder.py:\n",
    "https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "but was mildly modified because the original implementation is a bit confusing.\n",
    "I also tried to add as many comments as possible, my own understanding of what's\n",
    "going on.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import regex as re\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode\n",
    "    character that represents it visually. Some bytes have their appearance preserved\n",
    "    because they don't cause any trouble. These are defined in list bs. For example:\n",
    "    chr(33) returns \"!\", so in the returned dictionary we simply have d[33] -> \"!\".\n",
    "    However, chr(0), for example, is '\\x00', which looks ugly. So OpenAI maps these\n",
    "    bytes, into new characters in a range where chr() returns a single nice character.\n",
    "    So in the final dictionary we have d[0] -> 'Ä€' instead, which is just chr(0 + 2**8).\n",
    "    In particular, the space character is 32, which we can see by ord(' '). Instead,\n",
    "    this function will shift space (32) by 256 to 288, so d[32] -> 'Ä '.\n",
    "    So this is just a simple one-to-one mapping of bytes 0..255 into unicode characters\n",
    "    that \"look nice\", either in their original form, or a funny shifted character\n",
    "    like 'Ä€', or 'Ä ', etc.\n",
    "    \"\"\"\n",
    "    # the 188 integers that render fine in their original form and need no shifting\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"Â¡\"), ord(\"Â¬\")+1))+list(range(ord(\"Â®\"), ord(\"Ã¿\")+1))\n",
    "    cs = bs[:] # all integers b in bs will simply map to chr(b) in the output dict\n",
    "    # now get the representations of the other 68 integers that do need shifting\n",
    "    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            # if this byte is \"ugly\" then map it to the next available \"nice\" character\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    d = dict(zip(bs, cs))\n",
    "    return d\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"\n",
    "    Return all bigrams as a set of tuples, of consecutive elements in the iterable word.\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "\n",
    "    def __init__(self, encoder, bpe_merges):\n",
    "        # byte encoder/decoder\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        # bpe token encoder/decoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        # bpe merge list that defines the bpe \"tree\", of tuples (a,b) that are to merge to token ab\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        # the splitting pattern used for pre-tokenization\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions <-- original openai comment\n",
    "        \"\"\"\n",
    "        ok so what is this regex looking for, exactly?\n",
    "        python re reference: https://docs.python.org/3/library/re.html\n",
    "        - the vertical bars | is OR, so re.findall will chunkate text as the pieces match, from left to right\n",
    "        - '\\'s' would split up things like Andrej's -> (Andrej, 's)\n",
    "        - ' ?\\p{L}': optional space followed by 1+ unicode code points in the category \"letter\"\n",
    "        - ' ?\\p{N}': optional space followed by 1+ unicode code points in the category \"number\"\n",
    "        - ' ?[^\\s\\p{L}\\p{N}]+': optional space, then 1+ things that are NOT a whitespace, letter or number\n",
    "        - '\\s+(?!\\S)': 1+ whitespace characters (e.g. space or tab or etc) UNLESS they are followed by non-whitespace\n",
    "                       so this will consume whitespace characters in a sequence but exclude the last whitespace in\n",
    "                       that sequence. that last whitespace has the opportunity to then match the optional ' ?' in\n",
    "                       earlier patterns.\n",
    "        - '\\s+': 1+ whitespace characters, intended probably to catch a full trailing sequence of whitespaces at end of string\n",
    "        So TLDR:\n",
    "        - we are special casing a few common apostrophe constructs ('s, 't, 're, ...) and making those into separate tokens\n",
    "        - we then separate out strings into consecutive chunks of 1) letters, 2) numbers, 3) non-letter-numbers, 4) whitespaces\n",
    "        \"\"\"\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        self.cache = {}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        \"\"\"\n",
    "        this function uses self.bpe_ranks to iteratively merge all the possible bpe tokens\n",
    "        up the tree. token is a string of one individual 'word' (after regex tokenization)\n",
    "        and after byte encoding, e.g. 'Ä there'.\n",
    "        \"\"\"\n",
    "        # token is a string of one individual 'word', after byte encoding, e.g. 'Ä there'\n",
    "\n",
    "        # memoization, for efficiency\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "\n",
    "        word = tuple(token) # individual characters that make up the token, in a tuple\n",
    "        pairs = get_pairs(word) # get all bigrams\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # find the next lowest rank bigram that can be merged\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break # no more bigrams are eligible to be merged\n",
    "            first, second = bigram\n",
    "\n",
    "            # we will now replace all occurences of (first, second) in the list of current\n",
    "            # words into one merged token first_second, in the output list new_words\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "\n",
    "                # find the next occurence of first in the sequence of current words\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                # if this occurence is also followed by second, then merge them into one\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "\n",
    "            # all occurences of (first, second) have been merged to first_second\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "\n",
    "        # concat all words into a string, and use ' ' as the separator. Note that\n",
    "        # by now all characters have been byte encoded, guaranteeing that ' ' is\n",
    "        # not used in the actual data and is a 'special' delimiter character\n",
    "        word = ' '.join(word)\n",
    "\n",
    "        # cache the result and return\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\" string goes in, list of integers comes out \"\"\"\n",
    "        bpe_idx = []\n",
    "        # pre-tokenize the input text into string tokens (words, roughly speaking)\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        # process each token into BPE integers\n",
    "        for token in tokens:\n",
    "            # encode the token as a bytes (b'') object\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            # translate all bytes to their unicode string representation and flatten\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            # perform all the applicable bpe merges according to self.bpe_ranks\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            # translate all bpe tokens to integers\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            # extend our running list of all output integers\n",
    "            bpe_idx.extend(token_ix)\n",
    "        return bpe_idx\n",
    "\n",
    "    def encode_and_show_work(self, text):\n",
    "        \"\"\" debugging function, same as encode but returns all intermediate work \"\"\"\n",
    "        bpe_idx = []\n",
    "        parts = []\n",
    "        tokens = re.findall(self.pat, text)\n",
    "        for token in tokens:\n",
    "            token_bytes = token.encode('utf-8')\n",
    "            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)\n",
    "            token_merged = self.bpe(token_translated).split(' ')\n",
    "            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]\n",
    "            bpe_idx.extend(token_ix)\n",
    "            parts.append({\n",
    "                'token': token,\n",
    "                'token_bytes': token_bytes,\n",
    "                'token_translated': token_translated,\n",
    "                'token_merged': token_merged,\n",
    "                'token_ix': token_ix,\n",
    "            })\n",
    "        out = {\n",
    "            'bpe_idx': bpe_idx, # the actual output sequence\n",
    "            'tokens': tokens, # result of pre-tokenization\n",
    "            'parts': parts, # intermediates for each token part\n",
    "        }\n",
    "        return out\n",
    "\n",
    "    def decode(self, bpe_idx):\n",
    "        \"\"\" list of integers comes in, string comes out \"\"\"\n",
    "        # inverse map the integers to get the tokens\n",
    "        tokens_merged = [self.decoder[token] for token in bpe_idx]\n",
    "        # inverse the byte encoder, e.g. recovering 'Ä ' -> ' ', and get the bytes\n",
    "        tokens_flat = ''.join(tokens_merged)\n",
    "        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])\n",
    "        # recover the full utf-8 string\n",
    "        text = tokens_bytes.decode('utf-8', errors='replace')\n",
    "        return text\n",
    "\n",
    "def get_file(local_file, remote_file):\n",
    "    \"\"\" downloads remote_file to local_file if necessary \"\"\"\n",
    "    if not os.path.isfile(local_file):\n",
    "        print(f\"downloading {remote_file} to {local_file}\")\n",
    "        response = requests.get(remote_file)\n",
    "        open(local_file, \"wb\").write(response.content)\n",
    "\n",
    "def get_encoder():\n",
    "    \"\"\"\n",
    "    Returns an instance of the GPT BPE Encoder/Decoder\n",
    "    and handles caching of \"database\" files.\n",
    "    \"\"\"\n",
    "    home_dir = os.path.expanduser('~')\n",
    "    cache_dir = os.path.join(home_dir, '.cache', 'mingpt')\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    # load encoder.json that has the raw mappings from token -> bpe index\n",
    "    encoder_local_file = os.path.join(cache_dir, 'encoder.json')\n",
    "    encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'\n",
    "    get_file(encoder_local_file, encoder_remote_file)\n",
    "    with open(encoder_local_file, 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    assert len(encoder) == 50257 # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token\n",
    "\n",
    "    # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure\n",
    "    # in the form tuples (a, b), that indicate that (a, b) is to be merged to one token ab\n",
    "    vocab_local_file = os.path.join(cache_dir, 'vocab.bpe')\n",
    "    vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'\n",
    "    get_file(vocab_local_file, vocab_remote_file)\n",
    "    with open(vocab_local_file, 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    # light postprocessing: strip the version on first line and the last line is a blank\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    assert len(bpe_merges) == 50000 # 50,000 merged tokens\n",
    "\n",
    "    # construct the Encoder object and return\n",
    "    enc = Encoder(encoder, bpe_merges)\n",
    "    return enc\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\" PyTorch-aware class that wraps the Encoder above \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encoder = get_encoder()\n",
    "\n",
    "    def __call__(self, text, return_tensors='pt'):\n",
    "        # PyTorch only; here because we want to match huggingface/transformers interface\n",
    "        assert return_tensors == 'pt'\n",
    "        # single string input for now, in the future potentially a list of strings\n",
    "        assert isinstance(text, str)\n",
    "        # encode and create a \"batch dimension\" of 1\n",
    "        idx = [self.encoder.encode(text)]\n",
    "        # wrap into PyTorch tensor\n",
    "        out = torch.tensor(idx, dtype=torch.long)\n",
    "        return out\n",
    "\n",
    "    def decode(self, idx):\n",
    "        # ensure a simple 1D tensor for now\n",
    "        assert idx.ndim == 1\n",
    "        # decode indices to text\n",
    "        text = self.encoder.decode(idx.tolist())\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0acb6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there ...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/train.csv')\n",
    "text = data.full_text\n",
    "print(text[0][:200] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b410d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate encoder\n",
    "e = get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f108ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Labels:\n",
      " 0    [3.5, 3.5, 3.0, 3.0, 4.0, 3.0]\n",
      "1    [2.5, 2.5, 3.0, 2.0, 2.0, 2.5]\n",
      "2    [3.0, 3.5, 3.0, 3.0, 3.0, 2.5]\n",
      "3    [4.5, 4.5, 4.5, 4.5, 4.0, 5.0]\n",
      "4    [2.5, 3.0, 3.0, 3.0, 2.5, 2.5]\n",
      "Name: labels, dtype: object\n",
      "\n",
      "ðŸ”­ Encoded text:\n",
      " 0    [40, 892, 326, 2444, 561, 4414, 422, 4673, 379...\n",
      "1    [2215, 257, 1917, 318, 257, 1487, 345, 423, 28...\n",
      "2    [20266, 11, 32641, 198, 198, 1532, 334, 1487, ...\n",
      "3    [464, 1266, 640, 287, 1204, 318, 618, 345, 171...\n",
      "4    [18712, 719, 286, 23887, 460, 2928, 287, 584, ...\n",
      "Name: text_encoded, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's organize our labels for training\n",
    "data['labels'] = list(zip(data.cohesion.tolist(), data.syntax.tolist(),\n",
    "                          data.vocabulary.tolist(), data.phraseology.tolist(),\n",
    "                          data.grammar.tolist(), data.conventions.tolist()))\n",
    "data.labels = data.labels.map(lambda x: list(x))\n",
    "print(f'ðŸ“ Labels:\\n {data[\"labels\"].head()}\\n')\n",
    "\n",
    "# Let's clean the text a bit\n",
    "data['text_encoded'] = data['full_text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "\n",
    "# Now, let's encode the text using BPE class\n",
    "data.text_encoded = data.full_text.map(lambda x: e.encode(x))\n",
    "print(f'ðŸ”­ Encoded text:\\n {data.text_encoded.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32b13034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest sequence length: 938\n",
      "smallest sequence length: 2160\n"
     ]
    }
   ],
   "source": [
    "print(f'largest sequence length: {len(max(data.full_text))}')\n",
    "print(f'smallest sequence length: {len(min(data.full_text))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8613fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python38\\lib\\random.py:307: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[i], x[j] = x[j], x[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128,) (3128,)\n",
      "(391,) (391,)\n",
      "(392,) (392,)\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data.text_encoded)\n",
    "n1 = int(0.8*len(data.text_encoded))\n",
    "n2 = int(0.9*len(data.text_encoded))\n",
    "\n",
    "Xtr = data.text_encoded[:n1]\n",
    "Ytr = data.labels[:n1]\n",
    "Xdev = data.text_encoded[n1:n2]\n",
    "Ydev = data.labels[n1:n2]\n",
    "Xte = data.text_encoded[n2:]\n",
    "Yte = data.labels[n2:]\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xdev.shape, Ydev.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f058e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    \n",
    "    def __init__(self, split, xtrain, ytrain):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.xtrain = xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xtrain)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 500\n",
    "    \n",
    "    def block_size(self):\n",
    "        return 500\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xtrain[idx], self.ytrain[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d3f8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.13M\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Loader(split='train', xtrain=Xtr, ytrain=Ytr)\n",
    "dev_dataset = Loader(split='test', xtrain=Xdev, ytrain=Ydev)\n",
    "test_dataset = Loader(split='test', xtrain=Xte, ytrain=Yte)\n",
    "\n",
    "# create a GPT instance\n",
    "from minigpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = 500\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b442a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from minigpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91ce5726",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [55], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter_dt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_dt \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms; iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\miniGPT2\\minigpt\\trainer.py:85\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# fetch the next batch (x, y) and re-init iterator if needed\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    169\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9fa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
