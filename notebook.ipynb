{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69a0618",
   "metadata": {},
   "source": [
    "# GPT2 for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a15c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from minigpt.utils import set_seed\n",
    "set_seed(3407)\n",
    "\n",
    "from minigpt import bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3515212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there ...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/train.csv')\n",
    "text = data.full_text\n",
    "print(text[0][:200] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b09b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate encoder\n",
    "e = bpe.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8101db6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Labels:\n",
      " 0    [3.5, 3.5, 3.0, 3.0, 4.0, 3.0]\n",
      "1    [2.5, 2.5, 3.0, 2.0, 2.0, 2.5]\n",
      "2    [3.0, 3.5, 3.0, 3.0, 3.0, 2.5]\n",
      "3    [4.5, 4.5, 4.5, 4.5, 4.0, 5.0]\n",
      "4    [2.5, 3.0, 3.0, 3.0, 2.5, 2.5]\n",
      "Name: labels, dtype: object\n",
      "\n",
      "üî≠ Encoded text:\n",
      " 0    [40, 892, 326, 2444, 561, 4414, 422, 4673, 379...\n",
      "1    [2215, 257, 1917, 318, 257, 1487, 345, 423, 28...\n",
      "2    [20266, 11, 32641, 198, 198, 1532, 334, 1487, ...\n",
      "3    [464, 1266, 640, 287, 1204, 318, 618, 345, 171...\n",
      "4    [18712, 719, 286, 23887, 460, 2928, 287, 584, ...\n",
      "Name: text_encoded, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Let's organize our labels for training\n",
    "data['labels'] = list(zip(data.cohesion.tolist(), data.syntax.tolist(),\n",
    "                          data.vocabulary.tolist(), data.phraseology.tolist(),\n",
    "                          data.grammar.tolist(), data.conventions.tolist()))\n",
    "data.labels = data.labels.map(lambda x: list(x))\n",
    "print(f'üìê Labels:\\n {data[\"labels\"].head()}\\n')\n",
    "\n",
    "# Let's clean the text a bit\n",
    "data['text_encoded'] = data['full_text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "\n",
    "# Now, let's encode the text using BPE class\n",
    "data.text_encoded = data.full_text.map(lambda x: e.encode(x))\n",
    "print(f'üî≠ Encoded text:\\n {data.text_encoded.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f3fcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest sequence length: 938\n",
      "smallest sequence length: 2160\n"
     ]
    }
   ],
   "source": [
    "print(f'largest sequence length: {len(max(data.full_text))}')\n",
    "print(f'smallest sequence length: {len(min(data.full_text))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c4ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3128,) (3128,)\n",
      "(391,) (391,)\n",
      "(392,) (392,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python38\\lib\\random.py:307: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  x[i], x[j] = x[j], x[i]\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data.text_encoded)\n",
    "n1 = int(0.8*len(data.text_encoded))\n",
    "n2 = int(0.9*len(data.text_encoded))\n",
    "\n",
    "Xtr = data.text_encoded[:n1]\n",
    "Ytr = data.labels[:n1]\n",
    "Xdev = data.text_encoded[n1:n2]\n",
    "Ydev = data.labels[n1:n2]\n",
    "Xte = data.text_encoded[n2:]\n",
    "Yte = data.labels[n2:]\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xdev.shape, Ydev.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ee5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader(Dataset):\n",
    "    \n",
    "    def __init__(self, split, xtrain, ytrain):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.xtrain = xtrain\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xtrain)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 500\n",
    "    \n",
    "    def block_size(self):\n",
    "        return 500\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xtrain[idx], self.ytrain[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e94e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.13M\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Loader(split='train', xtrain=Xtr, ytrain=Ytr)\n",
    "dev_dataset = Loader(split='test', xtrain=Xdev, ytrain=Ydev)\n",
    "test_dataset = Loader(split='test', xtrain=Xte, ytrain=Yte)\n",
    "\n",
    "# create a GPT instance\n",
    "from minigpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = 500\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe91c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from minigpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a180fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
