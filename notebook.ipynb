{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69a0618",
   "metadata": {},
   "source": [
    "# GPT2 for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a15c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from minigpt.utils import set_seed\n",
    "from transformers import GPT2Tokenizer, GPT2Config, GPT2ForSequenceClassification\n",
    "\n",
    "set_seed(3407)\n",
    "\n",
    "# from minigpt import bpe\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3515212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there ...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('data/train.csv')\n",
    "text = data.full_text\n",
    "print(text[0][:200] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e17cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique chars:\n",
      " 94\n"
     ]
    }
   ],
   "source": [
    "# get vocab size\n",
    "raw_text = ''.join(data['full_text'].values.tolist())\n",
    "print('Num of unique chars:\\n', len(set(raw_text)))\n",
    "del raw_text # to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8251edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Byte Pair Encoder\n",
    "# e = bpe.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3129e0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      " tensor([[50256, 50256, 50256,  ...,   220,   220,   220],\n",
      "        [50256, 50256, 50256,  ...,   661,   892,    13],\n",
      "        [50256, 50256, 50256,  ...,   393,  4568,    13],\n",
      "        ...,\n",
      "        [50256, 50256, 50256,  ...,  2431,    13,   220],\n",
      "        [50256, 50256, 50256,  ...,   345,     0,   220],\n",
      "        [50256, 50256, 50256,  ...,   262,  6027,    13]])\n",
      "attention_mask\n",
      " tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]])\n",
      "labels\n",
      " tensor([3.5000, 2.5000, 3.0000,  ..., 2.5000, 4.0000, 3.5000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "# Instantiate tokenizer and pass `gpt2` to the `from_pretrained` method \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Select token to uses as `pad_token`\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Process text\n",
    "inputs = tokenizer(list(data.full_text), padding='longest', truncation=True,\n",
    "                  return_tensors=\"pt\", max_length=tokenizer.model_max_length)\n",
    "\n",
    "# Update the inputs with the associated encoded labels \n",
    "inputs.update({'labels':torch.tensor(data.cohesion)})\n",
    "\n",
    "print(\"input_ids\\n\", inputs['input_ids'])\n",
    "print(\"attention_mask\\n\", inputs['attention_mask'])\n",
    "print(\"labels\\n\", inputs['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "79945b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   40,   892,   326,  ..., 50256, 50256, 50256],\n",
       "        [ 2215,   257,  1917,  ..., 50256, 50256, 50256],\n",
       "        [20266,    11, 32641,  ..., 50256, 50256, 50256],\n",
       "        ...,\n",
       "        [    1,    32,  1917,  ..., 50256, 50256, 50256],\n",
       "        [ 7085,   661, 12546,  ..., 50256, 50256, 50256],\n",
       "        [ 5211,   345,   892,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([3.5000, 2.5000, 3.0000,  ..., 2.5000, 4.0000, 3.5000],\n",
       "       dtype=torch.float64)}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd1af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c1160e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3128, 1024]) torch.Size([3128])\n",
      "torch.Size([391, 1024]) torch.Size([391])\n",
      "torch.Size([392, 1024]) torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "# Create training, validation, and test sets\n",
    "\n",
    "n1 = int(0.8*len(inputs['input_ids']))\n",
    "n2 = int(0.9*len(inputs['input_ids']))\n",
    "         \n",
    "Xtr = inputs['input_ids'][:n1]\n",
    "Ytr = inputs['labels'][:n1]\n",
    "tr_mask = inputs['attention_mask'][:n1]\n",
    "\n",
    "Xdev = inputs['input_ids'][n1:n2]\n",
    "Ydev = inputs['labels'][n1:n2]\n",
    "dev_mask = inputs['attention_mask'][n1:n2]\n",
    "\n",
    "Xte = inputs['input_ids'][n2:]\n",
    "Yte = inputs['labels'][n2:]\n",
    "te_mask = inputs['attention_mask'][n2:]\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xdev.shape, Ydev.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "064be07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoads(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Y, Mask):\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "        self.mask = Mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids':self.x[idx],\n",
    "            'attention_mask':self.mask[idx],\n",
    "            'labels': self.y[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f7fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tr_mask\n",
    "del Xdev\n",
    "del dev_mask\n",
    "del Xte\n",
    "del te_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2a0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for input to transformer\n",
    "tr_loader = DataLoads(Xtr, Ytr, tr_mask)\n",
    "dev_loader = DataLoads(Xdev, Ydev, dev_mask)\n",
    "te_loader = DataLoads(Xte, Yte, te_mask)\n",
    "\n",
    "trainset = DataLoader(tr_loader, shuffle=True, batch_size=32)\n",
    "devset = DataLoader(dev_loader, shuffle=False, batch_size=32)\n",
    "teset = DataLoader(te_loader, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26d89ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate configuration class to store config params for GTP2Model\n",
    "model_config = GPT2Config.from_pretrained(\"gpt2\", num_labels=9)\n",
    "\n",
    "# Get Huggingface model\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", config=model_config)\n",
    "\n",
    "# Fix model with padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Load model to defined device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded to\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d2cda15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Model parameters\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                 lr = 2e-5,\n",
    "                 eps = 1e-8)\n",
    "\n",
    "\n",
    "# Training steps (num_batches * num_epochs)\n",
    "epochs = 10\n",
    "train_steps = len(trainset) * epochs\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps = 0,\n",
    "                                           num_training_steps = train_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b746300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop functions\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "\n",
    "    # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "    # Tracking variables.\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    # Total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "        # Add original labels - use later for evaluation.\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass.\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this a bert model function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer_.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler_.step()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Convert these logits to list of predicted labels values.\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Return all true labels and prediction for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ceb5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, device_):\n",
    "    \n",
    "    # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    #total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "        # add original labels\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple along with the logits. We will use logits\n",
    "            # later to to calculate training accuracy.\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # get predicitons to list\n",
    "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "            # update list\n",
    "            predictions_labels += predict_content\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Return all true labels and prediciton for future evaluations.\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2bdcf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350bfee142344d1894433ccd27e2d5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f976f0c569430ba29cdce233a2e23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 12.00 GiB total capacity; 9.46 GiB already allocated; 0 bytes free; 10.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on batches...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train_labels, train_predict, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m accuracy_score(train_labels, train_predict)\n\u001b[0;32m     10\u001b[0m     dev_labels, dev_predict, dev_loss \u001b[38;5;241m=\u001b[39m validation(devset, device)\n",
      "Cell \u001b[1;32mIn [26], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, optimizer_, scheduler_, device_)\u001b[0m\n\u001b[0;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Perform a forward pass (evaluate the model on this training batch).\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# This will return the loss (rather than the model output) because we\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# have provided the `labels`.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# The documentation for this a bert model function is here: \u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# The call to `model` always returns a tuple, so we need to pull the \u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# loss value out of the tuple along with the logits. We will use logits\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# later to calculate training accuracy.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1393\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1391\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1393\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1406\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1407\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:906\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    896\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    897\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    898\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    904\u001b[0m     )\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 906\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:406\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    404\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 406\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    415\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:347\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    345\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    350\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:223\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m# Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\u001b[39;00m\n\u001b[0;32m    222\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mtype(value\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 223\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 12.00 GiB total capacity; 9.46 GiB already allocated; 0 bytes free; 10.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(\"Training on batches...\")\n",
    "    train_labels, train_predict, train_loss = train(trainset, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "    \n",
    "    dev_labels, dev_predict, dev_loss = validation(devset, device)\n",
    "    dev_acc = accuracy_score(dev_labels, dev_predict)\n",
    "    \n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%\\\n",
    "          (train_loss, dev_loss, train_acc, dev_acc))\n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(dev_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(dev_acc)\n",
    "    \n",
    "    print(f\"train loss {all_loss['train_loss']}\")\n",
    "    print(f\"val loss {all_loss['val_loss']}\")\n",
    "    print(f\"train acc {all_loss['train_acc']}\")\n",
    "    print(f\"val acc {all_loss['val_acc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b490a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5eb6753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 20 10:36:49 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.15       Driver Version: 512.15       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:02:00.0  On |                  N/A |\n",
      "| 34%   53C    P8    23W / 350W |  11603MiB / 12288MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1320    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A      4344    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6268    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      6372    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      9136    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9736    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9912    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     10892    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13556    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     13608      C   C:\\Python38\\python.exe          N/A      |\n",
      "|    0   N/A  N/A     13632    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13960    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15116    C+G   C:\\WINDOWS\\System32\\dwm.exe     N/A      |\n",
      "|    0   N/A  N/A     16292    C+G   C:\\WINDOWS\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     18972    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     19120    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c0a0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Tensor: GPU pinned 1 × 1 × 1024 × 1024\n",
      "Tensor: GPU pinned \n",
      "Parameter: GPU pinned 50257 × 768\n",
      "Parameter: GPU pinned 1024 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 9 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 2304\n",
      "Parameter: GPU pinned 2304\n",
      "Parameter: GPU pinned 768 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Parameter: GPU pinned 768 × 3072\n",
      "Parameter: GPU pinned 3072\n",
      "Parameter: GPU pinned 3072 × 768\n",
      "Parameter: GPU pinned 768\n",
      "Total size: 137029644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hedronstone\\desktop\\minigpt2\\venv\\lib\\site-packages\\torch\\distributed\\distributed_c10d.py:181: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# View tensors still in use by the notebook\n",
    "# Source: https://forums.fast.ai/t/gpu-memory-not-being-freed-after-training-is-over/10265?replies_to_post_number=8\n",
    "\n",
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)\n",
    "    \n",
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5e2a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'collections': 702, 'collected': 2643, 'uncollectable': 0},\n",
       " {'collections': 63, 'collected': 972, 'uncollectable': 0},\n",
       " {'collections': 6, 'collected': 366, 'uncollectable': 0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "gc.get_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# class DataLoads(Dataset):\n",
    "    \n",
    "#     def __init__(self, X, Y):\n",
    "#         self.x = X\n",
    "#         self.y = Y\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "    \n",
    "#     def get_vocab_size(self):\n",
    "#         chars = list(itertools.chain.from_iterable(self.x[0:]))\n",
    "#         return len(set(chars))\n",
    "    \n",
    "#     def get_block_size(self):\n",
    "#         \"\"\"\n",
    "#         The length of the sequence that will feed into transformer,\n",
    "#         containing concatenated input and the output, but -1 because the transformer\n",
    "#         starts making predictions at the last input element.\n",
    "#         \"\"\"\n",
    "#         return self.length    \n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # inputs to the transformer\n",
    "#         X = torch.tensor(self.x[idx])\n",
    "#         Y = torch.tensor(self.y[idx])\n",
    "#         mask = torch.ones(len(self.x[0])).float()\n",
    "        \n",
    "#         return X, Y, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's organize our labels for training\n",
    "# data['labels'] = list(zip(data.cohesion.tolist(), data.syntax.tolist(),\n",
    "#                           data.vocabulary.tolist(), data.phraseology.tolist(),\n",
    "#                           data.grammar.tolist(), data.conventions.tolist()))\n",
    "\n",
    "# data.labels = data.labels.map(lambda x: list(x))\n",
    "# print(f'📐 Labels:\\n {data[\"labels\"].head()}\\n')\n",
    "\n",
    "# # Let's clean the text a bit\n",
    "# data['full_text'] = data['full_text'].apply(lambda x: x.replace('\\n', ' '))\n",
    "\n",
    "# # Now, let's encode the text using BPE class\n",
    "# data.text_encoded = data.full_text.map(lambda x: e.encode(x))\n",
    "# print(f'🔭 Encoded text:\\n {data.text_encoded.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3fcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'largest sequence length: {len(max(data.text_encoded))}')\n",
    "# print(f'smallest sequence length: {len(min(data.text_encoded))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e31b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GPT2 takes a maximum of 1028 tokens. Anything above that will cause index errors.\n",
    "# # So, Let's remove all samples above a given threshold\n",
    "# idxs = [i for i,j in enumerate(data.text_encoded) if len(j) < 800]\n",
    "\n",
    "# text = data.text_encoded[idxs].reset_index(drop=True)\n",
    "# labels = data.cohesion[idxs].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ca563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ToDo: Write function that adds padding to right of sequence\n",
    "# def add_pad(text):\n",
    "#     N = 800 # max sequence length\n",
    "#     a = text\n",
    "#     b = (N - len(a))\n",
    "#     a += [0] * b\n",
    "#     return a, b\n",
    "\n",
    "# text.map(lambda x: add_pad(x))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60affbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DataLoads(Xtr, Ytr)\n",
    "# dev_dataset = DataLoads(Xdev, Ydev)\n",
    "# test_dataset = DataLoads(Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e94e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a GPT instance\n",
    "# from minigpt.model import GPT\n",
    "\n",
    "# model_config = GPT.get_default_config()\n",
    "# model_config.model_type = 'gpt-nano'\n",
    "# model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "# model_config.block_size = 500\n",
    "# model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe91c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a Trainer object\n",
    "# from minigpt.trainer import Trainer\n",
    "\n",
    "# train_config = Trainer.get_default_config()\n",
    "# train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "# train_config.max_iters = 2000\n",
    "# train_config.num_workers = 0\n",
    "# trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a180fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_end_callback(trainer):\n",
    "#     if trainer.iter_num % 100 == 0:\n",
    "#         print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "# trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "# trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
